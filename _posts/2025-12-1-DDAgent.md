---
layout: post
title: "DDAgent: Dynamic Decision Agent for Druggability Analysis"
author: "Huiling Huang"
categories: project
tags: [project]
image: DDAgent.png 
---

  
DDAgent is a lightweight, decisionâ€‘driven agent that automates **logical reasoning** and **parameter selection** in a multiâ€‘step druggability analysis workflow.  
Instead of writing or modifying code, the agent watches what each step prints, interprets the results, and decides what should happen next based on domain rules.

I built DDAgent to sit on top of an existing scientific pipeline where all heavy numerical work is already implemented in Python utilities.  
The main challenge was to make the workflow **adaptive and interpretable** without turning the agent into yet another opaque â€œcode generatorâ€.

---

## ğŸ¯ Project Overview

In a typical druggability analysis, scientists run a sequence of scripts that:

- detect highâ€‘affinity residues  
- cluster hotspots  
- screen candidate molecules  
- refine and interpret results  

Each script writes verbose console output that a human needs to read and interpret before deciding whether to tweak thresholds, reâ€‘run, or move on.  
DDAgent takes over this coordination role.

At a high level, the agent:

- reads the text output from each step  
- determines whether the step â€œsucceededâ€ from a biological / logical perspective  
- asks an LLM for help when parameters need to be adjusted  
- reâ€‘runs or advances the pipeline accordingly  

Throughout the run, the agent records its decisions so that the whole reasoning process can be reviewed later.

---

## ğŸ§© Design Principles

1. **Decisionâ€‘only, no code generation**  
   DDAgent never writes new functions.  
   All analysis routines (e.g. for hotspot detection or clustering) remain in a stable `utils` module, which keeps the scientific codebase trustworthy and versionâ€‘controlled.

2. **Outputâ€‘first reasoning**  
   Every decision starts from actual printed output.  
   For example, if a step reports â€œno high affinity clusters foundâ€, the agent knows this is not simply a technical error but a **scientific signal** that may require relaxing a threshold.

3. **Transparent step history**  
   The agent stores, for every step:

   - the step name  
   - the raw output  
   - the parameters used  
   - the final decision taken  

   This makes the workflow auditable and suitable for research contexts where traceability matters.

4. **Modular, actionâ€‘based flow**  
   Each stage in the pipeline is implemented as a small â€œActionâ€ unit.  
   Actions encapsulate local decision logic but share a common context object, which keeps the overall system easy to extend with new steps.

---

## ğŸ§  How the Agent Behaves in Practice

A typical run looks like this:

1. **Run a scientific step**  
   An Action calls an existing analysis function (for example, a function that identifies highâ€‘affinity residues).

2. **Interpret the output**  
   The Action parses the console text and classifies the outcome: success, borderline, or failure.

3. **Adjust when needed**  
   If the outcome is unsatisfactory (for instance, no clusters are found or too many false positives appear), the Action asks the LLM to propose new parameter values within biologically reasonable bounds.

4. **Reâ€‘run or move forward**  
   The step is repeated with the updated parameters, or the pipeline advances to the next Action.  
   All of these decisions are stored in the shared context for later analysis.

Over time, this turns a fragile, linear script chain into a **resilient, feedbackâ€‘driven workflow**.

---

## ğŸ‘©ğŸ»â€ğŸ’» My Role & Contributions

In this project I was responsible for:

- Designing the **overall agent architecture** (controller, context, actions)  
- Defining the **decision rules** that connect scientific outcomes to parameter changes  
- Implementing the interaction between traditional Python utilities and the LLM  
- Ensuring that the system remains understandable for domain scientists, not just developers  

The result is a tool that lets researchers keep using their trusted analysis code, while benefiting from automated, LLMâ€‘assisted decision making.

---

## ğŸ“Œ Why DDAgent Matters

DDAgent targets a specific niche: scientific workflows where:

- the algorithms are already wellâ€‘established,  
- but the **interpretation and parameter tuning** are still manual and timeâ€‘consuming.

By formalising those decisions into an agent, the project:

- reduces repetitive manual oversight,  
- makes complex multiâ€‘step pipelines easier to reproduce, and  
- opens the door to more adaptive, dataâ€‘driven experimentation in druggability analysis.

Rather than replacing scientists, DDAgent acts as a **persistent decision assistant** that keeps track of what happened, why it happened, and what should happen next.

You can find the source code in [the repository](https://github.com/vvhuiling/DDagent).

